# model-zoo

1. Pytorch code of GRU "Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation" [code](https://github.com/wuqianliangsresearch/model-zoo/blob/master/seq2seq_mt/train_without_attention.py)
2. Pytorch code of soft attention "NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE" [code](https://github.com/wuqianliangsresearch/model-zoo/blob/master/seq2seq_mt/train_with_attention.py)
3. Pytorch code of soft attention "Effective Approaches to Attention-based Neural Machine Translation" [code](https://github.com/wuqianliangsresearch/model-zoo/blob/master/seq2seq_mt/train_with_local_global_attention.py)
In this [data page](https://nlp.stanford.edu/projects/nmt/)  page,  some preprocessed data for WMT14,WMT15 and codebase for "Effective Approaches to Attention-based Neural Machine Translation","Compression of Neural Machine Translation Models via Pruning","Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"
